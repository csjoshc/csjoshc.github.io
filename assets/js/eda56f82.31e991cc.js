"use strict";(self.webpackChunkdocusaurus_temp=self.webpackChunkdocusaurus_temp||[]).push([[3740],{5234:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Python/PythonforDataScience/ML","title":"ML","description":"\ud83c\udfe0 Home","source":"@site/docs/Python/PythonforDataScience/7_ML.md","sourceDirName":"Python/PythonforDataScience","slug":"/Python/PythonforDataScience/ML","permalink":"/docs/Python/PythonforDataScience/ML","draft":false,"unlisted":false,"editUrl":"https://github.com/csjoshc/csjoshc.github.io/tree/main/docs/Python/PythonforDataScience/7_ML.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Matplotlib","permalink":"/docs/Python/PythonforDataScience/Matplotlib"},"next":{"title":"ML_Clust","permalink":"/docs/Python/PythonforDataScience/ML_Clust"}}');var t=n(4848),r=n(8453);const a={},l="Machine Learning",o={},c=[{value:"Classification :",id:"classification-",level:2},{value:"kNN - k nearest neighbors",id:"knn---k-nearest-neighbors",level:3},{value:"Decision Tree",id:"decision-tree",level:3},{value:"Stopping the node splitting process",id:"stopping-the-node-splitting-process",level:4},{value:"Pros and Cons",id:"pros-and-cons",level:4},{value:"Clustering - organize similar items in data set into groups",id:"clustering---organize-similar-items-in-data-set-into-groups",level:2},{value:"Normalizing values",id:"normalizing-values",level:3},{value:"k-means clustering",id:"k-means-clustering",level:3},{value:"Choosing initial centroids",id:"choosing-initial-centroids",level:4},{value:"Evaluating cluster results",id:"evaluating-cluster-results",level:4},{value:"Choosing k",id:"choosing-k",level:4},{value:"Stopping criteria",id:"stopping-criteria",level:4},{value:"Naive Bayes",id:"naive-bayes",level:2},{value:"Regression: Predict a numeric value",id:"regression-predict-a-numeric-value",level:2},{value:"Linear regression",id:"linear-regression",level:3},{value:"Association Analysis - identify associations between items or events (co-occurence)",id:"association-analysis---identify-associations-between-items-or-events-co-occurence",level:2}];function d(e){const i={em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.p,{children:"\ud83c\udfe0 Home\n\ud83d\udc0d Python"}),"\n",(0,t.jsx)(i.header,{children:(0,t.jsx)(i.h1,{id:"machine-learning",children:"Machine Learning"})}),"\n",(0,t.jsx)(i.h2,{id:"classification-",children:"Classification :"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Binary or multi-variable (with more than two possible values)"}),"\n",(0,t.jsx)(i.li,{children:"A function that maps input to output based on parameters"}),"\n",(0,t.jsx)(i.li,{children:"The model adjusts the parameters to more accurately map input to output"}),"\n",(0,t.jsxs)(i.li,{children:["For classification, the observations exist on a feature space and are split up by ",(0,t.jsx)(i.strong,{children:"decision boundaries"})," into different regions, corresponding to different classifications"]}),"\n",(0,t.jsx)(i.li,{children:"Training phase - the algorithm uses the data to adjust the model to minimize error"}),"\n",(0,t.jsx)(i.li,{children:"Testing phase - the model is evaluated on unseen data"}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"knn---k-nearest-neighbors",children:"kNN - k nearest neighbors"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"samples with similar features likely belong to the same class"}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"decision-tree",children:"Decision Tree"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Use decision paths to segment observations"}),"\n",(0,t.jsx)(i.li,{children:"In a 2D feature space, a decision tree can be represented by a series of splits along either axis, that successively divide the data increasingly pure subsets"}),"\n",(0,t.jsx)(i.li,{children:"Attempt to subset the data so that the subsets are as pure as possible, containing samples of a single class"}),"\n",(0,t.jsx)(i.li,{children:"Repeat this process until you reach a plateau of homogeneity in the nodes"}),"\n",(0,t.jsx)(i.li,{children:"Start at the root node, and end up at leaf nodes at the bottom, each of which has a class label associated with it"}),"\n",(0,t.jsx)(i.li,{children:"At the branches in between, the answers to test conditions decide which branch to traverse along."}),"\n",(0,t.jsxs)(i.li,{children:["Each decision is made at the node point, in a ",(0,t.jsx)(i.strong,{children:"greedy algorithm"}),"\n",(0,t.jsx)(i.strong,{children:"Gini index"})," -  a measure of impurity measure that is minimized in order to determine the best splits"]}),"\n",(0,t.jsx)(i.li,{children:"Test variables and thresholds to split on"}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"stopping-the-node-splitting-process",children:"Stopping the node splitting process"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Splitting stops once there is no longer a significant increase in classification results with further splitting"}),"\n",(0,t.jsx)(i.li,{children:"Or, when a threshold purity for a node is reached (for example, 90%)"}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"pros-and-cons",children:"Pros and Cons"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"rectilinear when splitting on single variable"}),"\n",(0,t.jsx)(i.li,{children:"splitting on multiple variables simultanesously is computationally intensive"}),"\n",(0,t.jsx)(i.li,{children:"relatively simple to understand"}),"\n",(0,t.jsxs)(i.li,{children:["Greedy approach guarantees the best decision ",(0,t.jsx)(i.strong,{children:"at a node"}),", but not necessarily the best outcome for the dataset as a whole"]}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"clustering---organize-similar-items-in-data-set-into-groups",children:"Clustering - organize similar items in data set into groups"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Segment data into similar groups - similar to classification, but with no objective class labels, so Unsupervised"}),"\n",(0,t.jsx)(i.li,{children:"Segmentation of existing observations, classifying new data, anomaly detection (observations that aren't outliers in any one variable, but their combination makes them fall outside of existing cluster)"}),"\n",(0,t.jsx)(i.li,{children:"Intracluster differences are minimized, intercluster differences are maximized"}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"normalizing-values",children:"Normalizing values"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Since distance is used to assess similarity and dissimilarity, normalization of input variables is necessary to prevent any one from dominating the calculations."}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"k-means-clustering",children:"k-means clustering"}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["Select ",(0,t.jsx)(i.em,{children:"k"})," initial centroids"]}),"\n",(0,t.jsxs)(i.li,{children:["Assign each sample in the data set to the closest centroids","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"This is based on a distance calculation between the point and the centroid"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.li,{children:"Calculate cluster means, which become the new centroids"}),"\n",(0,t.jsx)(i.li,{children:"Repeat 2 & 3 until some stopping criteria is reached"}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"choosing-initial-centroids",children:"Choosing initial centroids"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Final clusters are sensitive to initial centroids. In order to solve this, repeat algorithm with randomized initial centroids to generate aggregate results."}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"evaluating-cluster-results",children:"Evaluating cluster results"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["Within cluster sum of squares","\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Error is the distance between the sample and centroid - square this error"}),"\n",(0,t.jsx)(i.li,{children:"Errors for all points in a cluster are summed"}),"\n",(0,t.jsx)(i.li,{children:"Clusters sums are summed"}),"\n",(0,t.jsx)(i.li,{children:"Within-Cluster Sum of Squared Error - WSSE"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.li,{children:"A cluster having a smaller WSSE is numerically better, but not more correct"}),"\n",(0,t.jsx)(i.li,{children:"Increasing k will always decrease WSSE"}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"choosing-k",children:"Choosing k"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Visualize for natural clusters, or depend on intended application/domain specific knowledge"}),"\n",(0,t.jsxs)(i.li,{children:["Data driven - generate metrics for a range of k values\nOne method is the elbow plot - see where the dropoff in WSSE as k increases begins to plateau, suggesting little benefit to adding further centroids\n",(0,t.jsx)(i.em,{children:"[Image:  - file not found]"})]}),"\n"]}),"\n",(0,t.jsx)(i.h4,{id:"stopping-criteria",children:"Stopping criteria"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"No changes to centroids"}),"\n",(0,t.jsx)(i.li,{children:"Cluster assignment changes per cycle fall below a threshold (such as 1%)"}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"naive-bayes",children:"Naive Bayes"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Probability of one event given another event"}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"regression-predict-a-numeric-value",children:"Regression: Predict a numeric value"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Used for forecasting, estimation, predictions future trends"}),"\n",(0,t.jsx)(i.li,{children:"Input variables can be numeric or categorical"}),"\n",(0,t.jsx)(i.li,{children:"Supervised, since the target label is a numeric value"}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"linear-regression",children:"Linear regression"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Simple but powerful model for a linear relationship between input and output; multiple linear regression can handle multiple inputs"}),"\n",(0,t.jsx)(i.li,{children:"Regression minimizes the error using the least squares method (square of the distance between the predicted and actual value, or the residual)"}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"association-analysis---identify-associations-between-items-or-events-co-occurence",children:"Association Analysis - identify associations between items or events (co-occurence)"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Supervised requires labeled target data - classification and regression"}),"\n",(0,t.jsx)(i.li,{children:"Unsupervised - no target or labeled data - Clustering and association analysis\nScikit-learn"}),"\n",(0,t.jsx)(i.li,{children:"end to end functionality for machine learning from data cleaning to modeling and analysis"}),"\n",(0,t.jsx)(i.li,{children:"cross validation, dimensionality reduction, visualization\nI'm curious how Skikit-learn functions match up against my experience working with R. Some of the best packages I've used in R include dplyr, tidyr, ggplot2"}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>a,x:()=>l});var s=n(6540);const t={},r=s.createContext(t);function a(e){const i=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:i},e.children)}}}]);