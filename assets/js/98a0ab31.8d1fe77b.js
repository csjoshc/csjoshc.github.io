"use strict";(self.webpackChunkdocusaurus_temp=self.webpackChunkdocusaurus_temp||[]).push([[5586],{5831:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>l,frontMatter:()=>r,metadata:()=>i,toc:()=>u});const i=JSON.parse('{"id":"Python/IntroCompThinkDataSci/unit5/unit5","title":"unit5","description":"\ud83c\udfe0 Home","source":"@site/docs/Python/IntroCompThinkDataSci/unit5/unit5.md","sourceDirName":"Python/IntroCompThinkDataSci/unit5","slug":"/Python/IntroCompThinkDataSci/unit5/","permalink":"/docs/Python/IntroCompThinkDataSci/unit5/","draft":false,"unlisted":false,"editUrl":"https://github.com/csjoshc/csjoshc.github.io/tree/main/docs/Python/IntroCompThinkDataSci/unit5/unit5.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"ps4","permalink":"/docs/Python/IntroCompThinkDataSci/unit4/ps4"},"next":{"title":"Introduction","permalink":"/docs/Python/ProbabilityandStatistics/Introduction"}}');var a=n(4848),o=n(8453);const r={},s="Machine Learning",c={},u=[];function h(t){const e={h1:"h1",header:"header",p:"p",...(0,o.R)(),...t.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.p,{children:"\ud83c\udfe0 Home\n\ud83d\udc0d Python"}),"\n",(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"machine-learning",children:"Machine Learning"})}),"\n",(0,a.jsx)(e.p,{children:"Machine learning is a process of generalizing historical data to build statistical models that can make meaningful and accurate predictions about future behavior. This process requires representing features, differences between features, an objective function to optimize, and a way to validate the performance. For supervised learning, validation is done by comparing the predicted to actual labels. For unsupervised learning, rather than using labels, the goal is to uncover latent patterns in the data.\nFor KNN, converting variables with larger dynamic ranges into smaller ranges can improve the Euclidean distance calculations.\nFor Kmeans, the greedy algorithm doesn't always find the best solution, but tries to approximate it. This solution can be a local minimum without being the global minimum. To get around this, we can iterate through the process repeatedly to elimiate the impact of the random starting locations."}),"\n",(0,a.jsx)(e.h1,{id:""})]})}function l(t={}){const{wrapper:e}={...(0,o.R)(),...t.components};return e?(0,a.jsx)(e,{...t,children:(0,a.jsx)(h,{...t})}):h(t)}},8453:(t,e,n)=>{n.d(e,{R:()=>r,x:()=>s});var i=n(6540);const a={},o=i.createContext(a);function r(t){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof t?t(e):{...e,...t}},[e,t])}function s(t){let e;return e=t.disableParentContext?"function"==typeof t.components?t.components(a):t.components||a:r(t.components),i.createElement(o.Provider,{value:e},t.children)}}}]);